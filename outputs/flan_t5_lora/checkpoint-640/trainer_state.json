{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 10.0,
  "eval_steps": 200,
  "global_step": 640,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.15810276679841898,
      "grad_norm": 3.458966016769409,
      "learning_rate": 4.2187499999999995e-05,
      "loss": 30.0353,
      "step": 10
    },
    {
      "epoch": 0.31620553359683795,
      "grad_norm": 4.122941493988037,
      "learning_rate": 8.906249999999999e-05,
      "loss": 30.3246,
      "step": 20
    },
    {
      "epoch": 0.4743083003952569,
      "grad_norm": 5.694674968719482,
      "learning_rate": 0.0001359375,
      "loss": 30.212,
      "step": 30
    },
    {
      "epoch": 0.6324110671936759,
      "grad_norm": 6.9866108894348145,
      "learning_rate": 0.00018281249999999998,
      "loss": 28.0722,
      "step": 40
    },
    {
      "epoch": 0.7905138339920948,
      "grad_norm": 7.273042678833008,
      "learning_rate": 0.0002296875,
      "loss": 24.7317,
      "step": 50
    },
    {
      "epoch": 0.9486166007905138,
      "grad_norm": 8.313742637634277,
      "learning_rate": 0.00027656249999999995,
      "loss": 20.4859,
      "step": 60
    },
    {
      "epoch": 1.0948616600790513,
      "grad_norm": 8.38718318939209,
      "learning_rate": 0.0002973958333333333,
      "loss": 15.9145,
      "step": 70
    },
    {
      "epoch": 1.2529644268774702,
      "grad_norm": 5.785854339599609,
      "learning_rate": 0.00029218749999999997,
      "loss": 12.6632,
      "step": 80
    },
    {
      "epoch": 1.4110671936758894,
      "grad_norm": 3.264533519744873,
      "learning_rate": 0.0002869791666666666,
      "loss": 9.6377,
      "step": 90
    },
    {
      "epoch": 1.5691699604743083,
      "grad_norm": 2.6747756004333496,
      "learning_rate": 0.0002817708333333333,
      "loss": 7.977,
      "step": 100
    },
    {
      "epoch": 1.7272727272727273,
      "grad_norm": 1.8831102848052979,
      "learning_rate": 0.00027656249999999995,
      "loss": 6.8594,
      "step": 110
    },
    {
      "epoch": 1.8853754940711462,
      "grad_norm": 1.6728452444076538,
      "learning_rate": 0.00027135416666666665,
      "loss": 6.0733,
      "step": 120
    },
    {
      "epoch": 2.0316205533596836,
      "grad_norm": 1.3102285861968994,
      "learning_rate": 0.0002661458333333333,
      "loss": 5.6014,
      "step": 130
    },
    {
      "epoch": 2.1897233201581026,
      "grad_norm": 1.5111396312713623,
      "learning_rate": 0.00026093749999999994,
      "loss": 5.2286,
      "step": 140
    },
    {
      "epoch": 2.3478260869565215,
      "grad_norm": 0.5243057012557983,
      "learning_rate": 0.00025572916666666664,
      "loss": 4.9813,
      "step": 150
    },
    {
      "epoch": 2.5059288537549405,
      "grad_norm": 0.889773964881897,
      "learning_rate": 0.0002505208333333333,
      "loss": 4.8009,
      "step": 160
    },
    {
      "epoch": 2.66403162055336,
      "grad_norm": 0.6638517379760742,
      "learning_rate": 0.0002453125,
      "loss": 4.6472,
      "step": 170
    },
    {
      "epoch": 2.822134387351779,
      "grad_norm": 0.4570452570915222,
      "learning_rate": 0.00024010416666666665,
      "loss": 4.5267,
      "step": 180
    },
    {
      "epoch": 2.9802371541501977,
      "grad_norm": 0.4363692104816437,
      "learning_rate": 0.00023489583333333332,
      "loss": 4.4268,
      "step": 190
    },
    {
      "epoch": 3.1264822134387353,
      "grad_norm": 0.433071106672287,
      "learning_rate": 0.0002296875,
      "loss": 4.3091,
      "step": 200
    },
    {
      "epoch": 3.2845849802371543,
      "grad_norm": 0.3973442316055298,
      "learning_rate": 0.00022447916666666664,
      "loss": 4.2244,
      "step": 210
    },
    {
      "epoch": 3.4426877470355732,
      "grad_norm": 0.36942192912101746,
      "learning_rate": 0.0002192708333333333,
      "loss": 4.1404,
      "step": 220
    },
    {
      "epoch": 3.600790513833992,
      "grad_norm": 0.45120587944984436,
      "learning_rate": 0.00021406249999999998,
      "loss": 4.0703,
      "step": 230
    },
    {
      "epoch": 3.758893280632411,
      "grad_norm": 0.36419129371643066,
      "learning_rate": 0.00020885416666666665,
      "loss": 3.9929,
      "step": 240
    },
    {
      "epoch": 3.91699604743083,
      "grad_norm": 0.41904571652412415,
      "learning_rate": 0.00020364583333333332,
      "loss": 3.9262,
      "step": 250
    },
    {
      "epoch": 4.063241106719367,
      "grad_norm": 0.4350750744342804,
      "learning_rate": 0.0001984375,
      "loss": 3.8486,
      "step": 260
    },
    {
      "epoch": 4.221343873517786,
      "grad_norm": 0.4412665367126465,
      "learning_rate": 0.00019322916666666664,
      "loss": 3.7789,
      "step": 270
    },
    {
      "epoch": 4.379446640316205,
      "grad_norm": 0.4686099886894226,
      "learning_rate": 0.00018802083333333333,
      "loss": 3.7116,
      "step": 280
    },
    {
      "epoch": 4.537549407114625,
      "grad_norm": 0.44696754217147827,
      "learning_rate": 0.00018281249999999998,
      "loss": 3.6448,
      "step": 290
    },
    {
      "epoch": 4.695652173913043,
      "grad_norm": 0.3767576813697815,
      "learning_rate": 0.00017760416666666665,
      "loss": 3.5845,
      "step": 300
    },
    {
      "epoch": 4.853754940711463,
      "grad_norm": 0.5017156004905701,
      "learning_rate": 0.00017239583333333332,
      "loss": 3.5238,
      "step": 310
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.8583008646965027,
      "learning_rate": 0.00016718749999999996,
      "loss": 3.4613,
      "step": 320
    },
    {
      "epoch": 5.158102766798419,
      "grad_norm": 0.4787554442882538,
      "learning_rate": 0.00016197916666666666,
      "loss": 3.4055,
      "step": 330
    },
    {
      "epoch": 5.316205533596838,
      "grad_norm": 0.6493223905563354,
      "learning_rate": 0.0001567708333333333,
      "loss": 3.3422,
      "step": 340
    },
    {
      "epoch": 5.474308300395257,
      "grad_norm": 0.6318682432174683,
      "learning_rate": 0.0001515625,
      "loss": 3.2891,
      "step": 350
    },
    {
      "epoch": 5.632411067193676,
      "grad_norm": 0.48014065623283386,
      "learning_rate": 0.00014635416666666665,
      "loss": 3.238,
      "step": 360
    },
    {
      "epoch": 5.790513833992095,
      "grad_norm": 0.7949735522270203,
      "learning_rate": 0.00014114583333333332,
      "loss": 3.2013,
      "step": 370
    },
    {
      "epoch": 5.948616600790514,
      "grad_norm": 0.5887160301208496,
      "learning_rate": 0.0001359375,
      "loss": 3.1551,
      "step": 380
    },
    {
      "epoch": 6.094861660079052,
      "grad_norm": 0.49959099292755127,
      "learning_rate": 0.00013072916666666666,
      "loss": 3.0938,
      "step": 390
    },
    {
      "epoch": 6.252964426877471,
      "grad_norm": 0.5348358154296875,
      "learning_rate": 0.00012552083333333333,
      "loss": 3.0536,
      "step": 400
    },
    {
      "epoch": 6.41106719367589,
      "grad_norm": 0.511809229850769,
      "learning_rate": 0.00012031249999999999,
      "loss": 3.0144,
      "step": 410
    },
    {
      "epoch": 6.569169960474309,
      "grad_norm": 0.4232887029647827,
      "learning_rate": 0.00011510416666666666,
      "loss": 2.984,
      "step": 420
    },
    {
      "epoch": 6.7272727272727275,
      "grad_norm": 0.4834578335285187,
      "learning_rate": 0.00010989583333333332,
      "loss": 2.9329,
      "step": 430
    },
    {
      "epoch": 6.8853754940711465,
      "grad_norm": 0.6503122448921204,
      "learning_rate": 0.00010468749999999999,
      "loss": 2.9058,
      "step": 440
    },
    {
      "epoch": 7.031620553359684,
      "grad_norm": 0.5744127035140991,
      "learning_rate": 9.947916666666665e-05,
      "loss": 2.8711,
      "step": 450
    },
    {
      "epoch": 7.189723320158103,
      "grad_norm": 0.5601890087127686,
      "learning_rate": 9.427083333333332e-05,
      "loss": 2.8319,
      "step": 460
    },
    {
      "epoch": 7.3478260869565215,
      "grad_norm": 0.4812161326408386,
      "learning_rate": 8.906249999999999e-05,
      "loss": 2.8039,
      "step": 470
    },
    {
      "epoch": 7.5059288537549405,
      "grad_norm": 0.5603985786437988,
      "learning_rate": 8.385416666666666e-05,
      "loss": 2.7736,
      "step": 480
    },
    {
      "epoch": 7.664031620553359,
      "grad_norm": 0.4586211144924164,
      "learning_rate": 7.864583333333333e-05,
      "loss": 2.7631,
      "step": 490
    },
    {
      "epoch": 7.822134387351778,
      "grad_norm": 0.5976036787033081,
      "learning_rate": 7.343749999999999e-05,
      "loss": 2.7293,
      "step": 500
    },
    {
      "epoch": 7.980237154150197,
      "grad_norm": 0.7743446826934814,
      "learning_rate": 6.822916666666666e-05,
      "loss": 2.7072,
      "step": 510
    },
    {
      "epoch": 8.126482213438734,
      "grad_norm": 0.4731794595718384,
      "learning_rate": 6.302083333333333e-05,
      "loss": 2.6935,
      "step": 520
    },
    {
      "epoch": 8.284584980237154,
      "grad_norm": 0.44024857878685,
      "learning_rate": 5.7812499999999996e-05,
      "loss": 2.6758,
      "step": 530
    },
    {
      "epoch": 8.442687747035572,
      "grad_norm": 0.7059057950973511,
      "learning_rate": 5.260416666666666e-05,
      "loss": 2.6598,
      "step": 540
    },
    {
      "epoch": 8.600790513833992,
      "grad_norm": 0.49798405170440674,
      "learning_rate": 4.7395833333333324e-05,
      "loss": 2.6398,
      "step": 550
    },
    {
      "epoch": 8.75889328063241,
      "grad_norm": 0.6401723623275757,
      "learning_rate": 4.2187499999999995e-05,
      "loss": 2.6309,
      "step": 560
    },
    {
      "epoch": 8.91699604743083,
      "grad_norm": 0.632197916507721,
      "learning_rate": 3.6979166666666667e-05,
      "loss": 2.6255,
      "step": 570
    },
    {
      "epoch": 9.063241106719367,
      "grad_norm": 0.6075682044029236,
      "learning_rate": 3.177083333333333e-05,
      "loss": 2.6279,
      "step": 580
    },
    {
      "epoch": 9.221343873517787,
      "grad_norm": 0.5065639019012451,
      "learning_rate": 2.65625e-05,
      "loss": 2.6081,
      "step": 590
    },
    {
      "epoch": 9.379446640316205,
      "grad_norm": 0.4613930881023407,
      "learning_rate": 2.1354166666666663e-05,
      "loss": 2.5942,
      "step": 600
    },
    {
      "epoch": 9.537549407114625,
      "grad_norm": 0.603603720664978,
      "learning_rate": 1.6145833333333334e-05,
      "loss": 2.5913,
      "step": 610
    },
    {
      "epoch": 9.695652173913043,
      "grad_norm": 0.6427236199378967,
      "learning_rate": 1.09375e-05,
      "loss": 2.5918,
      "step": 620
    },
    {
      "epoch": 9.853754940711463,
      "grad_norm": 0.5917905569076538,
      "learning_rate": 5.729166666666667e-06,
      "loss": 2.5798,
      "step": 630
    },
    {
      "epoch": 10.0,
      "grad_norm": 1.084086298942566,
      "learning_rate": 5.208333333333333e-07,
      "loss": 2.5853,
      "step": 640
    }
  ],
  "logging_steps": 10,
  "max_steps": 640,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 945954194718720.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
